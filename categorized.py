# -*- coding: utf-8 -*-
"""categorized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ckX4Tc6hvu7pw6rrhjTogbq0m-gT9DN8
"""

!pip install -q bertopic sentence-transformers hdbscan

import torch
if torch.cuda.is_available():
    device = "cuda"
    print("GPU is enabled.")
else:
    device = "cpu"
    print("GPU not detected, using CPU.")

import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from hdbscan import HDBSCAN
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/data_analysis.csv')
df = df[df['cleaned_text'].notnull()].reset_index(drop=True)

embedding_model = SentenceTransformer("all-mpnet-base-v2", device=device)  # Accurate + GPU support
docs = df['cleaned_text'].tolist()
embeddings = embedding_model.encode(docs, show_progress_bar=True, device=device)

vectorizer_model = CountVectorizer(
    stop_words="english", # removed ngrams/ bigrams
    min_df=1,  # filter very rare words, using default because higher values tend to mess up
    max_features=5000 #chaing it from default to 5000
)

hdbscan_model = HDBSCAN(min_cluster_size=5, min_samples=5, prediction_data=True)

topic_model = BERTopic(
    embedding_model=embedding_model,
    vectorizer_model=vectorizer_model,
    hdbscan_model=hdbscan_model,
    calculate_probabilities=True,
    verbose=True
)

topics, probs = topic_model.fit_transform(docs, embeddings)

df['topic_id'] = topics

df_filtered = df[df['topic_id'] != -1].reset_index(drop=True)

# new try
bar_chart = topic_model.visualize_barchart(top_n_topics=10)
bar_chart.show()

fig = topic_model.visualize_distribution(probs[0], min_probability=0.01)
fig.show()

named_topics = topic_model.get_topic_info()
df_named = df_filtered.merge(named_topics, how="left", left_on="topic_id", right_on="Topic")

print(f"Number of real topics (excluding outliers): {named_topics[named_topics['Topic'] != -1].shape[0]}")

def show_top_docs_per_topic(topic_num, n=5):
    sample = df_named[df_named['topic_id'] == topic_num][['title', 'cleaned_text']].head(n)
    print(f"\n--- Top {n} Documents for Topic {topic_num} ---")
    for i, row in sample.iterrows():
        print(f"\nTitle: {row['title']}\nText: {row['cleaned_text'][:500]}\n")

show_top_docs_per_topic(6, 3)

# Display the topic names and top words
named_topics = topic_model.get_topic_info()
print("=== Topic Summary ===")
print(named_topics[['Topic', 'Name', 'Count']].head(12))  # Adjust .head() as needed to show more topics

# total topics from the model
topic_model.get_topic_info()

#filtering confidence by threshold
confidence_threshold = 0.001
df_results = pd.DataFrame({
    'document': docs,
    'topic': topics,
    'confidence': probs[:,0]
})
df_filtered = df_results[df_results['confidence'] >= confidence_threshold]
df_filtered = df_filtered[df_filtered['topic'] != -1]
print(f"Original documents: {len(df_results)}")
print(f"Filtered documents: {len(df_filtered)}")

#filtered topic names
named_topics = topic_model.get_topic_info()
df_filtered_named = df_filtered.merge(named_topics, how='left', left_on='topic', right_on='Topic')

# top 10 posts per topic (trying 15 posts, if not working change it back to 15)
top_docs_per_topic = (
    df_filtered_named
    .sort_values(by='confidence', ascending=False)
    .groupby('topic')
    .head(15)
    .reset_index(drop=True)
)
top_docs_per_topic[['topic', 'Name', 'confidence', 'document']].head(20)  # Preview

topic_model.visualize_topics()

df_filtered = df_filtered.reset_index(drop=True)

#Merge back with original data
df_filtered_full = df_filtered.join(df)

df_filtered_named = df_filtered_full.merge(named_topics, how='left', left_on='topic', right_on='Topic')

# checking how many topics were there after thresholding
df_filtered_named['topic'].nunique()
df_filtered_named['topic'].value_counts()

with pd.ExcelWriter("bertopic_enriched_output_15posts.xlsx") as writer:
    df_filtered_named.to_excel(writer, sheet_name="All_Filtered", index=False)
    top_docs_per_topic.to_excel(writer, sheet_name="Top_10_Per_Topic", index=False)

"""### checking top 10 for filtered topics, so that the best matched categories alone are considered"""

show_top_docs_per_topic(6, 15)

"""note: topic 6 very well resembles stop word related issues"""

show_top_docs_per_topic(21, 15)

"""note: topic 21 well matches sentiment analysis category"""

show_top_docs_per_topic(20, 14)

"""note: topic 20 - classification using only 14 posts"""

show_top_docs_per_topic(61, 16)

"""note: topic 61 - well represents stemming totalling 16 posts"""

show_top_docs_per_topic(9, 69)

"""note: all 69 fine for topic 9 - topic name encoding strategies"""

show_top_docs_per_topic(24, 41)

"""note: NER topic 24, choosing top 40"""

show_top_docs_per_topic(70, 15)

"""note: topic 70 - clustering total 15 posts"""

show_top_docs_per_topic(89, 17)

"""note: above name - similarity"""

show_top_docs_per_topic(28, 41)

"""note: spell checker"""

show_top_docs_per_topic(78, 20)

"""note: Summarization"""

topic_model.save("bertopic_model", save_embedding_model=True)

